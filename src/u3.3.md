# Unit V: Big Data Analytics and Model Evaluation

## Clustering Algorithms

*   Clustering is an unsupervised machine learning task. You might also hear
    this referred to as cluster analysis because of the way this method works.

*   Using a clustering algorithm means you're going to give the algorithm a lot
    of input data with no labels and let it find any groupings in the data it
    can.

*   Those groupings are called clusters. A cluster is a group of data points
    that are similar to each other based on their relation to surrounding data
    points. Clustering is used for things like feature engineering or pattern
    discovery.

*   Types of clustering algorithms:

    *   **Distribution-based:** With a distribution-based clustering approach,
        all of the data points are considered parts of a cluster based on the
        probability that they belong to a given cluster.

    *   **Centroid-based:** Centroid-based clustering is the one you probably
        hear about the most. It's a little sensitive to the initial parameters
        you give it, but it's fast and efficient.

    *   **Hierarchical-based:** Hierarchical-based clustering is typically used
        on hierarchical data, like you would get from a company database or
        taxonomies. It builds a tree of clusters so everything is organized from
        the top-down.

### K-Means

*   K-means clustering is the most commonly used clustering algorithm. It's a
    centroid-based algorithm and the simplest unsupervised learning algorithm.

*   This algorithm tries to minimize the variance of data points within a
    cluster. It's also how most people are introduced to unsupervised machine
    learning.

*   K-means is best used on smaller data sets because it iterates over all of
    the data points. That means it'll take more time to classify data points if
    there are a large amount of them in the data set.

*   Since this is how k-means clusters data points, it doesn't scale well.

*   Implementation:

<!---->

    from numpy import unique
    from numpy import where
    from matplotlib import pyplot
    from sklearn.datasets import make_classification
    from sklearn.cluster import KMeans

    # initialize the data set we'll work with
    training_data, _ = make_classification(
        n_samples=1000,
        n_features=2,
        n_informative=2,
        n_redundant=0,
        n_clusters_per_class=1,
        random_state=4
    )

    # define the model
    kmeans_model = KMeans(n_clusters=2)

    # assign each data point to a cluster
    dbscan_result = dbscan_model.fit_predict(training_data)

    # get all of the unique clusters
    dbscan_clusters = unique(dbscan_result)

    # plot the DBSCAN clusters
    for dbscan_cluster in dbscan_clusters:
        # get data points that fall in this cluster
        index = where(dbscan_result == dbscan_clusters)
        # make the plot
        pyplot.scatter(training_data[index, 0], training_data[index, 1])

    # show the DBSCAN plot
    pyplot.show()

### Hierarchical Clustering

Hierarchical clustering, also known as hierarchical cluster analysis, is an
algorithm that groups similar objects into groups called clusters. The endpoint
is a set of clusters, where each cluster is distinct from each other cluster,
and the objects within each cluster are broadly similar to each other.

[More Details](https://www.displayr.com/what-is-hierarchical-clustering/)

### Time-series analysis

Time series analysis is a specific way of analyzing a sequence of data points
collected over an interval of time. In time series analysis, analysts record
data points at consistent intervals over a set period of time rather than just
recording the data points intermittently or randomly. However, this type of
analysis is not merely the act of collecting data over time.

What sets time series data apart from other data is that the analysis can show
how variables change over time. In other words, time is a crucial variable
because it shows how the data adjusts over the course of the data points as well
as the final results. It provides an additional source of information and a set
order of dependencies between the data.

[More Details](https://www.tableau.com/learn/articles/time-series-analysis)

## Introduction to Text Analysis

Text analysis (TA) is a machine learning technique used to automatically extract
valuable insights from unstructured text data. Companies use text analysis tools
to quickly digest online data and documents, and transform them into actionable
insights.

You can us text analysis to extract specific information, like keywords, names,
or company information from thousands of emails, or categorize survey responses
by sentiment and topic.

[More Details](https://monkeylearn.com/text-analysis/)

### Text-preprocessing

Text preprocessing is an approach for cleaning and preparing text data for use
in a specific context. Developers use it in almost all natural language
processing (NLP) pipelines, including voice recognition software, search engine
lookup, and machine learning model training. It is an essential step because
text data can vary. From its format (website, text message, voice recognition)
to the people who create the text (language, dialect), there are plenty of
things that can introduce noise into your data.

The ultimate goal of cleaning and preparing text data is to reduce the text to
only the words that you need for your NLP goals.

[More
Details](https://www.codecademy.com/learn/text-preprocessing/modules/nlp-text-preprocessing)

### Bag of words

*   The bag-of-words model is a way of representing text data when modeling text
    with machine learning algorithms.

*   The bag-of-words model is simple to understand and implement and has seen
    great success in problems such as language modeling and document
    classification.

*   A bag-of-words model, or BoW for short, is a way of extracting features from
    text for use in modeling, such as with machine learning algorithms.

*   The approach is very simple and flexible, and can be used in a myriad of
    ways for extracting features from documents.

*   A bag-of-words is a representation of text that describes the occurrence of
    words within a document. It involves two things:
    1.  A vocabulary of known words.
    2.  A measure of the presence of known words.

*   It is called a “bag” of words, because any information about the order or
    structure of words in the document is discarded. The model is only concerned
    with whether known words occur in the document, not where in the document.

*   [More
    Details](https://machinelearningmastery.com/gentle-introduction-bag-words-model/)

### TF-IDF and topics

*   TF-IDF stands for term frequency-inverse document frequency and it is a
    measure, used in the fields of information retrieval (IR) and machine
    learning, that can quantify the importance or relevance of string
    representations (words, phrases, lemmas, etc)  in a document amongst a
    collection of documents (also known as a corpus).

*   TF-IDF can be broken down into two parts TF (term frequency) and IDF
    (inverse document frequency).

*   Term frequency works by looking at the frequency of a particular term you
    are concerned with relative to the document. There are multiple measures, or
    ways, of defining frequency:
    *   Number of times the word appears in a document (raw count).
    *   Term frequency adjusted for the length of the document (raw count of
        occurences divided by number of words in the document).
    *   Logarithmically scaled frequency (e.g. log(1 + raw count)).
    *   Boolean frequency (e.g. 1 if the term occurs, or 0 if the term does not
        occur, in the document).

### Need and Introduction to social network analysis

*   Social network analysis (SNA) is the process of investigating social
    structures through the use of networks and graph theory.\[1] It
    characterizes networked structures in terms of nodes (individual actors,
    people, or things within the network) and the ties, edges, or links
    (relationships or interactions) that connect them.

*   Need for social network analysis:

    *   Social network analysis (SNA) can provide insight into social influences
        within teams, and identify cultural issues.

    *   The research conducted for SNA is interested in individuals, but the
        analysis itself focuses on connectivity: how individuals collaborate.

    *   SNA has been used as a strategic approach to team building, and to
        understand how team building can change the dynamics of an
        organisation’s social network.

*   [More Details](https://en.wikipedia.org/wiki/Social_network_analysis)

### Introduction to business analysis.

Business analysis is a professional discipline of identifying business needs and
determining solutions to business problems. Solutions often include a
software-systems development component, but may also consist of process
improvements, organizational change or strategic planning and policy
development.

## Model Evaluation and Selection

Model evaluation is a method of assessing the correctness of models on test
data. The test data consists of data points that have not been seen by the model
before.

Model selection is a technique for selecting the best model after the individual
models are evaluated based on the required criteria.

[More
Details](https://neptune.ai/blog/the-ultimate-guide-to-evaluation-and-selection-of-models-in-machine-learning)

### Metrics for Evaluating Classifier Performance

Evaluation metrics are tied to machine learning tasks. There are different
metrics for the tasks of classification and regression. Some metrics, like
precision-recall, are useful for multiple tasks. Classification and regression
are examples of supervised learning, which constitutes a majority of machine
learning applications. Using different metrics for performance evaluation, we
should be able to improve our model’s overall predictive power before we roll it
out for production on unseen data. Without doing a proper evaluation of the
Machine Learning model by using different evaluation metrics, and only depending
on accuracy, can lead to a problem when the respective model is deployed on
unseen data and may end in poor predictions.

[More
Details](https://www.analyticsvidhya.com/blog/2021/07/metrics-to-evaluate-your-classification-model-to-take-the-right-decisions/)

### Holdout Method and Random Sub sampling

Hold-Out Method is a resampling technique with a basic idea of dividing the
training dataset into two parts i.e. train and test. On one part(train) you try
to train the model and on the second part(test) i.e. the data which is unseen
for the model, you make the prediction and check how well your model works on
it.

Random subsampling performs K data splits of the entire sample. For each data
split, a fixed number of observations is chosen without replacement from the
sample and kept aside as the test data.

[More
Details](https://blog.ineuron.ai/Hold-Out-Method-Random-Sub-Sampling-Method-3MLDEXAZML)

### Parameter Tuning and Optimization

In machine learning, hyperparameter optimization or tuning is the problem of
choosing a set of optimal hyperparameters for a learning algorithm. A
hyperparameter is a parameter whose value is used to control the learning
process. By contrast, the values of other parameters (typically node weights)
are learned.

[More Details](https://en.wikipedia.org/wiki/Hyperparameter_optimization)

### Result Interpretation

Data interpretation refers to the process of using diverse analytical methods to
review data and arrive at relevant conclusions. The interpretation of data helps
researchers to categorize, manipulate, and summarize the information in order to
answer critical questions.

[More
Details](https://www.datapine.com/blog/data-interpretation-methods-benefits-problems/)

### Clustering and Time-series analysis using Scikit- learn

Clustering methods, one of the most useful unsupervised ML methods, used to find
similarity & relationship patterns among data samples. After that, they cluster
those samples into groups having similarity based on features. Clustering
determines the intrinsic grouping among the present unlabeled data, that’s why
it is important.

The Scikit-learn library have sklearn.cluster to perform clustering of unlabeled
data. Under this module scikit-leran have the following clustering methods −

[More
Details](https://www.tutorialspoint.com/scikit_learn/scikit_learn_clustering_methods.htm)

### sklearn.metrics

*   The sklearn.metrics module implements functions assessing prediction error
    for specific purposes. These metrics are detailed in sections on
    Classification metrics, Multilabel ranking metrics, Regression metrics and
    Clustering metrics.

*   Finally, Dummy estimators are useful to get a baseline value of those
    metrics for random predictions.

*   For the most common use cases, you can designate a scorer object with the
    scoring parameter. All scorer objects follow the convention that higher
    return values are better than lower return values. Thus metrics which
    measure the distance between the model and the data, like
    metrics.mean\_squared\_error, are available as neg\_mean\_squared\_error
    which return the negated value of the metric.

*   The module sklearn.metrics also exposes a set of simple functions measuring
    a prediction error given ground truth and prediction:

    *   functions ending with \_score return a value to maximize, the higher the
        better.

    *   functions ending with \_error or \_loss return a value to minimize, the
        lower the better. When converting into a scorer object using
        make\_scorer, set the greater\_is\_better parameter to False (True by
        default; see the parameter description below).

*   [More
    Details](https://scikit-learn.org/stable/modules/model_evaluation.html)

### Confusion matrix

*   A confusion matrix is a table that is often used to describe the performance
    of a classification model (or "classifier") on a set of test data for which
    the true values are known. The confusion matrix itself is relatively simple
    to understand, but the related terminology can be confusing.

*   I wanted to create a "quick reference guide" for confusion matrix
    terminology because I couldn't find an existing resource that suited my
    requirements: compact in presentation, using numbers instead of arbitrary
    variables, and explained both in terms of formulas and sentences.

*   **NEEDS REFACTORING**

<!-- TODO: Refer 18th prelime question  -->

*   [More
    Details](https://www.dataschool.io/simple-guide-to-confusion-matrix-terminology/)

### AUC-ROC Curves

AUC-ROC is the valued metric used for evaluating the performance in
classification models. The AUC-ROC metric clearly helps determine and tell us
about the capability of a model in distinguishing the classes. The judging
criteria being - Higher the AUC, better the model. AUC-ROC curves are frequently
used to depict in a graphical way the connection and trade-off between
sensitivity and specificity for every possible cut-off for a test being
performed or a combination of tests being performed. The area under the ROC
curve gives an idea about the benefit of using the test for the underlying
question. AUC - ROC curves are also a performance measurement for the
classification problems at various threshold settings.

[More
Details](https://analyticsindiamag.com/understanding-the-auc-roc-curve-in-machine-learning-classification/)

### Elbow plot

In cluster analysis, the elbow method is a heuristic used in determining the
number of clusters in a data set. The method consists of plotting the explained
variation as a function of the number of clusters, and picking the elbow of the
curve as the number of clusters to use. The same method can be used to choose
the number of parameters in other data-driven models, such as the number of
principal components to describe a data set.

[More Details](https://en.wikipedia.org/wiki/Elbow_method_\(clustering\))
