# Unit VI: Data Visualization and Hadoop

## Introduction to Data Visualization

Data visualization is actually a set of data points and information that are
represented graphically to make it easy and quick for user to understand. Data
visualization is good if it has a clear meaning, purpose, and is very easy to
interpret, without requiring context. Tools of data visualization provide an
accessible way to see and understand trends, outliers, and patterns in data by
using visual effects or elements such as a chart, graphs, and maps.

[More Details](https://www.geeksforgeeks.org/short-note-on-data-visualization/)

## Challenges to Big data visualization

*   Scalability and dynamics are two major challenges in visual analytics.

*   Big Data visualization can be an extremely powerful business capability, but
    before an organization can take advantage of it some key issues need to be
    addressed. These include:

    *   **Availability of visualization specialists:** Many Big Data
        visualization tools are designed to be easy enough for anyone in an
        organization to use, often suggesting appropriate Big Data visualization
        examples  for the data sets under analysis. But to get the most out of
        some tools  it may be necessary to employ a specialist in big data
        visualization techniques who can select the best data sets and
        visualization styles to ensure the data is exploited to the maximum.

    *   **Visualization hardware resources:** Under the hood, Big Data
        visualization is essentially a computing task, and the ability to carry
        out this task quickly – to enable organizations to make decisions in a
        timely manner using real-time data – may require powerful computer
        hardware, fast storage systems, or even a move to cloud. That means Big
        Data visualization initiatives are as much an IT project as a management
        project.

    *   **Data quality:** The insights that can be drawn from Big Data
        visualization are only as accurate as the data that is being visualized:
        if it is inaccurate or out of date then the value of any insights is
        questionable. That means people and processes need to be put in place
        to manage corporate data, metadata, data sources, and any
        transformations or data cleaning that are performed before storage.

## Types of data visualization

*   Some of the most common types of data visualization chart and graph formats
    include:
    *   Column Chart
    *   Bar Graph
    *   Line Graph
    *   Heat Map
    *   Pie Chart
    *   Scatter Plot Chart
    *   Bubble Chart
    *   Area Chart
    *   Stacked Bar Graph
    *   Stacked Column Chart
    *   Dual Axis Chart
    *   Mekko Chart
    *   Waterfall Chart
    *   Bullet Graph
    *   Funnel Chart

*   Also read the section below for data visualization techniques in detail.

## Data Visualization Techniques

[More details and visual representations of
graphs](https://online.hbs.edu/blog/post/data-visualization-techniques)

## Visualizing Big Data

*   Big data visualization refers to the implementation of more contemporary
    visualization techniques to illustrate the relationships within data.
    Visualization tactics include applications that can display real-time
    changes and more illustrative graphics, thus going beyond pie, bar and other
    charts.

## Tools used in Data Visualization

*   Data visualization tools are software applications that render information
    in a visual format such as a graph, chart, or heat map for data analysis
    purposes. Such tools make it easier to understand and work with massive
    amounts of data. With effective data visualization tools, people can make
    data-driven decisions without having to spend valuable time trying to
    wrangle raw data into an interpretable format. Properly configured, data
    visualization software does that work for you, sifting through vast stores
    of information to present only the most meaningful, relevant data.

*   Some commonly used tools are:

    *   Zoho Analytics: Focusing on ease of use – a particularly key attribute
        as data tools grow – Zoho analytics is a self service option. Meaning
        that users will not need the assistance of IT staff or professional data
        scientists to glean insight from data.

    *   IBM Cognos Analytics: Driven by their commitment to Big Data, IBM’s
        analytics package offers a variety of self service options to more
        easily identify insight.

    *   QlikSense and QlikView: The Qlik solution touts its ability to perform
        the more complex analysis that finds hidden insights.

    *   Microsoft PowerBI: The Power BI tools enables you to connect with
        hundreds of data sources, then publish reports on the Web and across
        mobile devices.

    *   Oracle Visual Analyzer: A web-based tool, Visual Analyzer allows
        creation of curated dashboards to help discover correlations and
        patterns in data.

    *   SAP Lumira: Calling it “self service data visualization for everyone,”
        Lumira allows you to combine your visualizations into storyboards.

    *   SAS Visual Analytics: The SAS solution promotes its “scalability and
        governance,” along with dynamic visuals and flexible deployment options.

    *   Tableau Desktop: Tableau’s interactive dashboards allow you to “uncover
        hidden insights on the fly,” and power users can manage metadata to make
        the most of disparate data sources.

    *   TIBCO Spotfire: Offers analytics software as a service, and touts itself
        as a solution that “scales from a small team to the entire
        organization.”

*   [More details and visual representations of
    graphs](https://www.simplilearn.com/data-visualization-tools-article)

## Hadoop ecosystem

*   Hadoop Ecosystem is a platform or a suite which provides various services to
    solve the big data problems. It includes Apache projects and various
    commercial tools and solutions. There are four major elements of Hadoop i.e.
    HDFS, MapReduce, YARN, and Hadoop Common. Most of the tools or solutions are
    used to supplement or support these major elements. All these tools work
    collectively to provide services such as absorption, analysis, storage and
    maintenance of data etc.

![Hadoop Ecosystem](pictures/hadoop_eco.png)

*   Following are the components that collectively form a Hadoop ecosystem:
    *   **HDFS:** Hadoop Distributed File System
    *   **YARN:** Yet Another Resource Negotiator
    *   **MapReduce:** Programming based Data Processing
    *   **Spark:** In-Memory data processing
    *   **PIG, HIVE:** Query based processing of data services
    *   **HBase:** NoSQL Database
    *   **Mahout, Spark MLLib:** Machine Learning algorithm libraries
    *   **Solar, Lucene:** Searching and Indexing
    *   **Zookeeper:** Managing cluster
    *   **Oozie:** Job Scheduling

*   [More Details](https://www.geeksforgeeks.org/hadoop-ecosystem/)

### Map Reduce

*   By making the use of distributed and parallel algorithms, MapReduce makes it
    possible to carry over the processing’s logic and helps to write
    applications which transform big data sets into a manageable one.

*   MapReduce makes the use of two functions i.e. Map() and Reduce() whose task
    is:

    1.  Map() performs sorting and filtering of data and thereby organizing them
        in the form of group. Map generates a key-value pair based result which is
        later on processed by the Reduce() method.
    2.  Reduce(), as the name suggests does the summarization by aggregating the
        mapped data. In simple, Reduce() takes the output generated by Map() as
        input and combines those tuples into smaller set of tuples.

### Pig

*   Pig was basically developed by Yahoo which works on a pig Latin language,
    which is Query based language similar to SQL.
*   It is a platform for structuring the data flow, processing and analyzing
    huge data sets.
*   Pig does the work of executing commands and in the background, all the
    activities of MapReduce are taken care of. After the processing, pig stores
    the result in HDFS.
*   Pig Latin language is specially designed for this framework which runs on
    Pig Runtime. Just the way Java runs on the JVM.
*   Pig helps to achieve ease of programming and optimization and hence is a
    major segment of the Hadoop Ecosystem.

### Hive

*   With the help of SQL methodology and interface, HIVE performs reading and
    writing of large data sets. However, its query language is called as HQL
    (Hive Query Language).
*   It is highly scalable as it allows real-time processing and batch processing
    both. Also, all the SQL datatypes are supported by Hive thus, making the
    query processing easier.
*   Similar to the Query Processing frameworks, HIVE too comes with two
    components: JDBC Drivers and HIVE Command Line.
*   JDBC, along with ODBC drivers work on establishing the data storage
    permissions and connection whereas HIVE Command line helps in the processing
    of queries.

## Analytical techniques used in Big data visualization

[Refer Data Visualization Techniques Section
above](u3.4.md#types-of-data-visualization)

## Data Visualization using Python

### Line plot

Matplotlib is a data visualization library in Python. The pyplot, a sublibrary
of matplotlib, is a collection of functions that helps in creating a variety of
charts.  Line charts are used to represent the relation between two data X and Y
on a different axis. Here we will see some of the examples of a line chart in
Python :

[More Details](https://www.geeksforgeeks.org/line-chart-in-matplotlib-python/)

### Scatter plot

[Read here](https://www.w3schools.com/python/python_ml_scatterplot.asp)

### Histogram

A histogram is basically used to represent data provided in a form of some
groups.It is accurate method for the graphical representation of numerical data
distribution.It is a type of bar plot where X-axis represents the bin ranges
while Y-axis gives information about frequency.

[More
Details](https://www.geeksforgeeks.org/plotting-histogram-in-python-using-matplotlib/)

### Density plot

Density Plot is a type of data visualization tool. It is a variation of the
histogram that uses ‘kernel smoothing’ while plotting the values. It is a
continuous and smooth version of a histogram inferred from a data.

Density plots uses Kernel Density Estimation (so they are also known as Kernel
density estimation plots or KDE) which is a probability density function. The
region of plot with a higher peak is the region with maximum data points
residing between those values.

Density plots can be made using pandas, seaborn, etc. In this article, we will
generate density plots using Pandas. We will be using two datasets of the
Seaborn Library namely – ‘car\_crashes’ and ‘tips’.

[More
Details](https://www.geeksforgeeks.org/density-plots-with-pandas-in-python/)

### Box-plot

A Box Plot is also known as Whisker plot is created to display the summary of
the set of data values having properties like minimum, first quartile, median,
third quartile and maximum. In the box plot, a box is created from the first
quartile to the third quartile, a vertical line is also there which goes through
the box at the median. Here x-axis denotes the data to be plotted while the
y-axis shows the frequency distribution.

[More
Details](https://www.geeksforgeeks.org/box-plot-in-python-using-matplotlib/)
